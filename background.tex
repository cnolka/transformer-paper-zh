减少序列计算的目标同样构成了扩展神经GPU\citep{extendedngpu}、ByteNet\citep{NalBytenet2017}和ConvS2S\citep{JonasFaceNet2017}的基础，这些模型均使用卷积神经网络作为基本构建模块，并行计算所有输入和输出位置的隐藏表示。在这些模型中，关联两个任意输入或输出位置信号所需的操作次数随位置间距离的增长而增加：ConvS2S为线性增长，ByteNet为对数增长。这使得学习远距离位置之间的依赖关系变得更加困难\citep{hochreiter2001gradient}。在Transformer中，这一操作次数被减少到一个常数级别，尽管这是以降低有效分辨率为代价的（因为对注意力加权的位置进行了平均），我们通过第~\ref{sec:attention}节所述的多头注意力机制来抵消这种影响。

自注意力（Self-attention），有时也称为内部注意力（intra-attention），是一种关联单个序列中不同位置的注意力机制，目的是计算该序列的表示。自注意力已成功应用于多种任务，包括阅读理解、抽象摘要、文本蕴含学习以及任务无关的句子表示学习\citep{cheng2016long, decomposableAttnModel, paulus2017deep, lin2017structured}。

端到端记忆网络基于循环注意力机制而非序列对齐的循环，并已被证明在简单语言问答和语言建模任务上表现良好\citep{sukhbaatar2015}。

然而，据我们所知，Transformer是第一个完全依赖自注意力来计算其输入和输出表示的转换模型，而没有使用序列对齐的RNN或卷积。在接下来的章节中，我们将描述Transformer的架构，解释自注意力的动机，并讨论其相对于\citep{neural_gpu, NalBytenet2017}和\citep{JonasFaceNet2017}等模型的优势。


%\citep{JonasFaceNet2017} 报告了在英语-德语、英语-法语和英语-罗马尼亚语机器翻译任务上达到的新SOTA结果。

%例如，在机器翻译中，我们必须同时从输入词和先前的输出词中提取信息，以准确地翻译一个输出词。注意力层\citep{bahdanau2014neural}能够以较低的计算成本连接非常多的位置，这使其成为具有竞争力的机器翻译循环模型中的关键组成部分。

%那么一个很自然的问题是：“我们能否用注意力来替代循环？”。这样的模型将兼具注意力的计算效率和跨位置通信的能力。在这项工作中，我们证明了纯注意力模型在机器翻译上表现非常出色，在英德和英法翻译上取得了新的SOTA结果，并且可以在xyz架构上在$2$天内完成训练。

%在\citep{sutskever14, bahdanau2014neural, cho2014learning}提出的开创性模型之后，循环模型已成为序列建模和序列到序列转换任务的主导解决方案。许多努力，例如\citep{wu2016google,luong2015effective,jozefowicz2016exploring}，通过循环编码器-解码器和循环语言模型不断推进机器翻译和语言建模的边界。最近的工作\citep{shazeer2017outrageously}成功地将条件计算的能力与序列模型相结合，以更低的计算成本训练用于机器翻译的超大模型，并推动了SOTA水平。

%循环模型为计算的每个时间步$t$计算一个隐藏状态向量$h_t$。$h_t$是时间$t$的输入和前一隐藏状态$h_t$的函数。这种对前一隐藏状态的依赖阻止了所有时间步的同时处理，反而需要一长串的顺序操作。在实践中，这导致计算效率大大降低，因为在现代计算硬件上，对大批量数据的单次操作远比对小批量的多次操作快得多。序列越长，问题越严重。尽管顺序计算在推理时并非严重瓶颈，因为自回归地生成每个输出都需要所有先前的输出，但无法同时计算所有输出位置的分数阻碍了我们在大型数据集上快速训练模型。尽管像\citep{Kuchaiev2017Factorization}这样令人印象深刻的工作能够通过因子化技巧显著加速LSTM的训练，我们仍然受限于对序列长度的线性依赖。

%如果模型能够仅使用输入和输出来计算每个时间步的隐藏状态，那么在训练期间它将摆脱对前一时间步结果的依赖。这一思路是近期诸如马尔可夫神经GPU\citep{neural_gpu}、ByteNet\citep{NalBytenet2017}和ConvS2S\citep{JonasFaceNet2017}等工作的基础，这些工作都使用卷积神经网络作为构建块来同时计算所有时间步的隐藏表示，从而实现了$O(1)$的顺序时间复杂度。\citep{JonasFaceNet2017}报告了在英语-德语、英语-法语和英语-罗马尼亚语机器翻译任务上达到的新SOTA结果。

%精确序列预测的一个关键组成部分是对跨位置通信的建模。例如，在机器翻译中，我们必须同时从输入词和先前的输出词中提取信息，以准确地翻译一个输出词。注意力层\citep{bahdanau2014neural}能够以较低的计算成本（同样具有$O(1)$的顺序时间复杂度）连接非常多的位置，这使其成为机器翻译中循环编码器-解码器架构的关键组成部分。那么一个很自然的问题是：“我们能否用注意力来替代循环？”。这样的模型将兼具注意力的计算效率和跨位置通信的能力。在这项工作中，我们证明了纯注意力模型在机器翻译上表现非常出色，在英德和英法翻译上取得了新的SOTA结果，并且可以在xyz架构上在$2$天内完成训练。



%注意：Facebook的模型在这方面并不比RNN更好，因为它需要与所需通信距离成比例的层数。ByteNet更有前途，因为它需要层数是对数级的（ByteNet有SOTA结果吗）？

%注意：注意力层能够以$O(1)$顺序操作的低计算成本连接非常多的位置。这就是为什么编码器-解码器注意力在序列到序列模型中如此成功的原因。那么，很自然地，也可以使用注意力来连接同一序列的时间步。

%注意：我不会说长序列在推理时不是问题。如果我们能在没有长序列的情况下进行推理，那就太好了。我们可以稍后说明，虽然我们的训练图是恒定深度的，但由于模型的自回归特性，我们的模型在推理时解码器部分仍然需要顺序操作。

%\begin{table}[h!]
%\caption{当序列长度小于通道深度时，注意力模型对于跨位置通信是相当高效的。$n$代表序列长度，$d$代表通道深度。}
%\label{tab:op_complexities}
%\begin{center}
%\vspace{-5pt}
%\scalebox{0.75}{

%\begin{tabular}{l|c|c|c}
%\hline \hline
%层类型 & 感受野 & 计算复杂度 & 顺序操作数 \\
%\hline
%逐点前馈网络 & $1$ & $O(n \cdot d^2)$ & $O(1)$ \\
%\hline
%循环层 & $n$ & $O(n \cdot d^2)$ & $O(n)$ \\
%\hline
%卷积层 & $r$ & $O(r \cdot n \cdot d^2)$ & $O(1)$ \\
%\hline
%卷积层（可分离）& $r$ & $O(r \cdot n \cdot d + n \cdot d^2)$ & $O(1)$ \\
%\hline
%注意力层 & $r$ & $O(r \cdot n \cdot d)$ & $O(1)$ \\
%\hline \hline
%\end{tabular}
%}
%\end{center}
%\end{table}
