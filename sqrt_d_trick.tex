\section*{点积注意力中缩放因子的理由}

在第~\ref{sec:scaled-dot-prod}节中，我们介绍了缩放点积注意力，其中我们将点积按 $\sqrt{d_k}$ 缩放。在本节中，我们将给出这个缩放因子的大致理由。如果我们假设 $q$ 和 $k$ 是 $d_k$ 维向量，其分量是均值为 $0$、方差为 $1$ 的独立随机变量，那么它们的点积 $q \cdot k = \sum_{i=1}^{d_k} u_iv_i$ 的均值为 $0$，方差为 $d_k$。由于我们希望这些值的方差为 $1$，因此我们除以 $\sqrt{d_k}$。

%对于任意两个 $d_k$ 维向量 $\vec{u}$ 和 $\vec{v}$，其维度是独立的，点积的均值和方差将是各维度均值和方差乘积的求和，即 $E[<\vec{u},\vec{v}>] = \sum_{i=1}^{d_k} E[u_i]E[v_i]$，且 $E[(<\vec{u},\vec{v}>-E[<\vec{u},\vec{v}>])^2] = \sum_{i=1}^{d_k} E[({u_i}-E[u_i])^2] E[({v_i}-E[v_i])^2]$。层归一化鼓励每个维度的均值和方差分别为 $0$ 和 $1$，从而导致点积的均值分别为 $0$ 和 $d_k$。因此，按 $\sqrt{d_k}$ 缩放鼓励 logits 也被归一化。

\iffalse

在本节中，我们将给出这个缩放因子的大致理由，即我们将证明对于任意两个向量 $\vec{u}$ 和 $\vec{v}$，其方差和均值分别为 $1$ 和 $0$，点积的方差和均值分别为 $d_k$ 和 $0$。因此，除以 $\sqrt{d_k}$ 确保注意力 logits 的每个分量都被归一化。每个 Transformer 层中的重复层归一化鼓励 $\vec{u}$ 和 $\vec{v}$ 被归一化。

\begin{align*}
    E[<\vec{u},\vec{v}>] & =  \sum_k E[u_i v_i] &\text{由期望的线性性} \\
    & =\sum_k E[u_i]E[v_i] & \text{假设独立性} \\
    & = 0
\end{align*}

\begin{align*}
    E[(<\vec{u},\vec{v}>-E[<\vec{u},\vec{v}>])^2]  & = E[(<\vec{u},\vec{v}>)^2] - E[<\vec{u},\vec{v}>]^2 \\
    & = E[(<\vec{u},\vec{v}>)^2] \\
    & =  \sum_k E[{u_i}^2] E[{v_i}^2] &\text{由期望的线性性和独立性} \\
    & = d_k
\end{align*}

\fi
